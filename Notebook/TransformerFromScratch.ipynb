{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQO31YfKVHG9"
      },
      "source": [
        "#Lets implement 'BART', a seq-seq transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpVXFIyoVhqG"
      },
      "source": [
        "BART was originally introduced in the paper called, 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension' by by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019.\n",
        "\n",
        "For more information please refer to: [HuggingFace BART ](https://huggingface.co/docs/transformers/en/model_doc/bart#implementation-notes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGGGxmEgWDzB"
      },
      "source": [
        "The goal of this project is not to beat any benchmark or create a state-of-the-art performing model; rather, it is to learn, write from scratch, and implement the minor details that we don't get to interface with when using APIs or pretrained models.\n",
        "\n",
        "I wanted to write everything in pure PyTorch, but in some places, I had to use Hugging Face libraries (like for the tokenizer and evaluation). So, in this single notebook, we will train an encoder-decoder model on the objective of text summarization. Note that we will not be using any pretrained weights, so the performance of our model won't be that good, but that is not the goal, either.\n",
        "\n",
        "The training objective is that the model, given an 'Article' and its 'Summary,' should be trained to correctly output the summary. We will use the CNN/DailyMail dataset provided by Hugging Face. You can read more about it here:[CNN/DailyMail Dataset HuggingFace](https://huggingface.co/datasets/abisee/cnn_dailymail)\n",
        "\n",
        "As training begins, you'll observe the model's generations as it learns, gradually combining characters to form words and then sentences. I also aimed for configurability, so all parameters are tunable. You can adjust the model's size, experiment with different layers, and observe their impact on performance. Hope you have fun :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Lyi_GjXS3Y"
      },
      "source": [
        "## Components to Implement:\n",
        "1. Loading the Dataset\n",
        "2. Defining the model config\n",
        "2. Token Embeddings\n",
        "3. Positional Encodings\n",
        "4. Multi-Head Attention\n",
        "5. Encoder Block\n",
        "6. Decoder Block\n",
        "7. Transformer\n",
        "8. Sampler (Greedy Decoding || Beam Search)\n",
        "9. Training Pipeline\n",
        "10. Evaluation Script\n",
        "\n",
        "My inspiration:\n",
        "1. [Attention is All you Need](https://arxiv.org/abs/1706.03762)\n",
        "2. [BART](https://arxiv.org/abs/1910.13461)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwnZFJDOYGZv"
      },
      "source": [
        "### All the necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TVEeaTkKYmZT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKzinyHvLKej"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate as evaluation_metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BartTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import PreTrainedModel, GenerationConfig, GenerationMixin\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "import math\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt2cOl6sY9PD"
      },
      "source": [
        "### loading dataset utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u_U29hO7YYQA"
      },
      "outputs": [],
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 articles,\n",
        "                 summaries,\n",
        "                 tokenizer=BartTokenizer.from_pretrained(\"facebook/bart-base\"),\n",
        "                 max_length=512):\n",
        "        self.articles = articles\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        # tokenize the article and summary\n",
        "        article_encodings = self.tokenizer(article,\n",
        "                                           max_length=self.max_length,\n",
        "                                           padding=\"max_length\",\n",
        "                                           truncation=True)\n",
        "        summary_encodings = self.tokenizer(summary,\n",
        "                                           max_length=self.max_length,\n",
        "                                           padding=\"max_length\",\n",
        "                                           truncation=True)\n",
        "\n",
        "        # this masking is needed to distinguish between padding and actual tokens\n",
        "        article_mask = article_encodings[\"attention_mask\"]\n",
        "        summary_mask = summary_encodings[\"attention_mask\"]\n",
        "\n",
        "        # perform teacher forcing\n",
        "        # decoder input ids are what we feed into the decoder\n",
        "        # labels is what we expect the decoder output to be\n",
        "        ''' Breif summary of how teacher forcing works:\n",
        "        In the decoder, we feed the input ids with the last predicted token removed\n",
        "        The first token in the label is removed since it is the start token and we want to predict the next token\n",
        "        So say the input sentence is \"Hello, how are you?\" with token ids: [<start> , 8 , 9 , 10 , 11 , 12 , <end>]\n",
        "        The decoder input ids are: [<start> , 8 , 9 , 10 , 11 , 12]\n",
        "        The labels are: [8 , 9 , 10 , 11 , 12 , <end>]\n",
        "        So when '<start>' is fed into the decoder, the output should be 8, so the label is 8\n",
        "        The next input to the decoder is 8, so the output should be 9, so the label is 9\n",
        "        This continues until the last token is reached\n",
        "        '''\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(article_encodings[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(article_mask),\n",
        "            \"decoder_input_ids\": torch.tensor(summary_encodings[\"input_ids\"][:-1]),\n",
        "            \"decoder_attention_mask\": torch.tensor(summary_mask[:-1]),\n",
        "            \"labels\": torch.tensor(summary_encodings[\"input_ids\"][1:])\n",
        "        }\n",
        "\n",
        "\n",
        "def load_cnn_dailymail(batch_size,\n",
        "                       max_length):\n",
        "    dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "    train_dataset = SummarizationDataset(\n",
        "        articles=dataset[\"train\"][\"article\"],\n",
        "        summaries=dataset[\"train\"][\"highlights\"],\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=max_length)\n",
        "\n",
        "    val_dataset = SummarizationDataset(\n",
        "        articles=dataset[\"validation\"][\"article\"],\n",
        "        summaries=dataset[\"validation\"][\"highlights\"],\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=max_length)\n",
        "\n",
        "    test_dataset = SummarizationDataset(\n",
        "        articles=dataset[\"test\"][\"article\"],\n",
        "        summaries=dataset[\"test\"][\"highlights\"],\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=max_length)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLXa0hYhZ0Z6"
      },
      "source": [
        "#### before going any furthur, lets also define some model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B93PzgJVZvyZ"
      },
      "outputs": [],
      "source": [
        "model_config = {\"model_name\":\"bart_seq2seq\",\n",
        "                \"vocab_size\": 50265,               # defines the size of the token vocab\n",
        "                \"max_seq_len\": 512,                # max sequence of length that the model can process\n",
        "                \"d_model\": 512,                    # dimension of embedding and hidden states throughout the model\n",
        "                \"encoder_layers\": 12,              # number of encoder/decoder layers in the transformer\n",
        "                \"decoder_layers\": 12,              # number of encoder/decoder layers in the transformer\n",
        "                \"encoder_attention_heads\": 8,      # number of attention heads in multihead attention\n",
        "                \"decoder_attention_heads\": 8,      # number of attention heads in multihead attentino\n",
        "                \"encoder_ff_dim\": 1024,            # hidden dim size in FF network\n",
        "                \"decoder_ff_dim\": 1024,            # hidden dim size in FF network\n",
        "                \"dropout\": 0.1,                    # general dropout rate throughout the model\n",
        "                \"pad_token_id\": 1,                 # special token ids used in generation and processing\n",
        "                \"begin_sequence_token_id\": 0,      # special token ids used in generation and processing\n",
        "                \"end_sequence_token_id\": 2,        # special token ids used in generation and processing\n",
        "                \"beam_size\": 4,                    # control the num of paths in beam search\n",
        "                \"batch_size\": 16,                  # batch size (how many examples are batched together to train in parallel)\n",
        "                \"learning_rate\": 3e-5,             # learning rate decides how big of a step you should take in the GD\n",
        "                \"warmup_steps\": 500,               # used in the learning rate scheduler\n",
        "                \"max_grad_norm\": 1.0,              # maximum gradient norm used for clipping (used in the training loop  )\n",
        "                \"num_epochs\": 1,                   # full passes through the training data\n",
        "                \"num_samples\":500,                 # how many samples to generate in the evaluation (for vibe check)\n",
        "                \"eval_sample\":3,                   # num samples that we get in the evaluation during training\n",
        "                \"log_every\":1000,                  # defines logging frequency\n",
        "                \"start_eval_gen\":100}              # again used for logging frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_qAn9VsCs_"
      },
      "source": [
        "#### **a note on what is 'warm up steps' ?**\n",
        "\n",
        "*So when training neural networks, you can keep the learning rate constant, but it often leads to better, stable, faster convergence if you 'smartly' adjust the learning rate. You can use exponential decay learning rate, step decay, cosine annealing, etc. But i decided to use linear learning rate scheduler with warmup.*\n",
        "\n",
        "*warmup basically works in this way, you start with a very small value and gradually increase to a predefined maximum value over a certain number of steps (which is defined by the warmup steps). Once the warmup is complete, the learning rate follows some decay schedule (like linear in this case but can be exponential as well).* *Read this [they have some nice visuals]*(https://docs.anyscale.com/llms/finetuning/guides/modify_hyperparams/)\n",
        "\n",
        "\n",
        "#### **a note on 'max grad norm' ?**\n",
        "\n",
        "*gradient clipping is used in our training loop to avoid exploding gradients*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb6Z1D5xasyq"
      },
      "source": [
        "### embeddings  (this includes both the token embeddings and positional embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCYEyOpxajpF"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements sinusoidal positional encoding as described in \"Attention is All You Need\"\n",
        "\n",
        "    This will add positional information to the input embeddings since transformer architectures\n",
        "    don't inherently understand the sequence order\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int,\n",
        "                 max_sequence_length: int,\n",
        "                 dropout_prob: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        position = torch.arange(max_sequence_length).unsqueeze(1)\n",
        "        # implement the sinusoidal encoding (formula at page 6 of the paper)\n",
        "        denominator = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "\n",
        "        pe = torch.zeros(1, max_sequence_length, embedding_dim)\n",
        "        pe[0,:,0::2] = torch.sin(position * denominator)\n",
        "        pe[0,:,1::2] = torch.cos(position * denominator)\n",
        "\n",
        "        # since the positional encoding are not learned, we can simply store them in model state\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input Tensor of shape [batch size, seq len, embedding dim]\n",
        "        Returns:\n",
        "            Tensor with positional encoding added\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds the input tokens into a vector space of dimension d_model\n",
        "    Also we have to scale the embeddings by sqrt(d_model) as described in the paper page 5\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embedding_dim: int):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input Tensor of shape [batch size, seq len]\n",
        "\n",
        "        Returns:\n",
        "            Embedded tokens scaled by sqrt(d_model) [batch size, seq len, embedding dim]\n",
        "        \"\"\"\n",
        "        return self.embedding(x) * math.sqrt(self.embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIt6yPSfbOjJ"
      },
      "source": [
        "### code up the multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vL_SN7zbEsn"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention mechanism similar to what is described in 'Attention is all you need'\n",
        "    You linearly project each of the queries, keys and values with different learned projections\n",
        "    And in each head, self attention is applied, the result are concatenated and projected to the output\n",
        "\n",
        "    This basically helps the model to jointly attend to information from different positions, with different represenatational subspaces\n",
        "    For example, if we have a sentence \"He went to the bank to get some money, and later went for a walk along the river bank\"\n",
        "    The model should be able to attend to the words \"bank\" in different ways:\n",
        "    - \"bank\" as a place to get money\n",
        "    - \"bank\" as a place to walk along the river\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        dropout: float):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        # dk is the dimension of each head's key, query and value\n",
        "\n",
        "        # linear layers for queries, keys and values projections\n",
        "        self.q_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "        # e.g 8 heads * 64 dk -> 512 d model\n",
        "        self.out_proj = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def attention(\n",
        "        self,\n",
        "        q: torch.Tensor,\n",
        "        k: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        The popular attention mechanism\n",
        "\n",
        "        Args:\n",
        "            query: [batch size, num heads, seq len q, d_k]\n",
        "            key: [batch size, num heads, seq len k, d_k]\n",
        "            value: [batch size, num heads, seq len v, d_k]\n",
        "            mask: [batch size, 1, seq len q, seq len k]\n",
        "        Returns:\n",
        "            Tensor of shape [batch size, num heads, seq len, d_k]\n",
        "        \"\"\"\n",
        "        similarity = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            similarity = similarity.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = F.softmax(similarity, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        attention_scores = torch.matmul(attention_weights, v)\n",
        "        return attention_scores, attention_weights\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: [batch size, seq len q, d model]\n",
        "            key: [batch size, seq len k, d model]\n",
        "            value: [batch size, seq len v, d model]\n",
        "            mask: [batch size, 1, seq len q, seq len k]\n",
        "\n",
        "        Returns:\n",
        "            Two tensors output and attention weights\n",
        "            output: [batch size, num heads, seq len q, d model]\n",
        "            attention weights: [batch size, num heads, seq len q, seq len k]\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # view each of q,k,v to [batch size, seq len, num heads, dk]\n",
        "        # so if (32, 10, 512) -> (32, 10, 8, 64)\n",
        "        q = self.q_proj(query)\n",
        "        k = self.k_proj(key)\n",
        "        v = self.v_proj(value)\n",
        "\n",
        "        # chunk it for each head\n",
        "        # why transpose?\n",
        "        # [batch size, seq len, num heads, dk] -> [batch size, num heads, seq len, dk]\n",
        "        q = q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        attention_scores, attention_weights = self.attention(q, k, v, mask)\n",
        "\n",
        "        # combine the heads (sounds kinda weird)\n",
        "        # lets break down the math\n",
        "        # output -> [batch size, num heads, seq len, dk]\n",
        "        # revoke the transpose -> [batch size, seq len, num heads, dk]\n",
        "        # view it back to [batch size, seq len, num heads * dk]\n",
        "        combined = attention_scores.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # project it back to d_model\n",
        "        projected = self.out_proj(combined)\n",
        "\n",
        "        return projected, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jwPeDUQbeVp"
      },
      "source": [
        "### code up the 'Encoder Block'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAkpF2FtbYfW"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple/single encoder block:\n",
        "    - Multi-head attention mechanism\n",
        "    - Feed-forward neural network\n",
        "    - Layer normalization\n",
        "    NOTE: GELU instead of ReLU is used as per the BART paper\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 config):\n",
        "        super().__init__()\n",
        "        self.d_model = config[\"d_model\"]\n",
        "        self.encoder_attention_heads = config[\"encoder_attention_heads\"]\n",
        "        self.dropout = config[\"dropout\"]\n",
        "        self.encoder_ff_dim = config[\"encoder_ff_dim\"]\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(\n",
        "            d_model=self.d_model,\n",
        "            num_heads=self.encoder_attention_heads,\n",
        "            dropout=self.dropout,\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(self.d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.d_model)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.d_model, self.encoder_ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.encoder_ff_dim, self.d_model),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "    def forward(self,\n",
        "                x,\n",
        "                attention_mask=None):\n",
        "        # tidbit: residual connection is used to avoid the vanishing gradient problem\n",
        "        # also the way we have implemented this is called \"pre layer normalization\" since BART paper uses this\n",
        "        residual = x\n",
        "\n",
        "        # attention with residual connection\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # convert from (batch size, seq len) to (batch size, 1, 1, seq len)\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            # now expand to (batch size, 1, seq len, seq len)\n",
        "            seq_len = attention_mask.size(3)\n",
        "            attention_mask = attention_mask.expand(-1, -1, seq_len, -1)\n",
        "\n",
        "        x, _ = self.self_attention(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            mask=attention_mask\n",
        "        )\n",
        "\n",
        "        x = x + residual\n",
        "        # feedforward with residual connection\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = x + residual\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elxEvqC7byTK"
      },
      "source": [
        "### code up the 'Decoder Block'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAsJ6IV0bx2D"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single decoder block for BART:\n",
        "    - Masked multi-head attention (No peeking)\n",
        "    - Multi-head cross attention with encoder outputs\n",
        "    - Feed forward NN\n",
        "    - layer Normalization\n",
        "    - Residual connections\n",
        "    NOTE: Uses Pre-LN arch like the encoder also uses GELU\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 config):\n",
        "        super().__init__()\n",
        "        self.d_model = config[\"d_model\"]\n",
        "        self.decoder_attention_heads = config[\"decoder_attention_heads\"]\n",
        "        self.decoder_ff_dim = config[\"decoder_ff_dim\"]\n",
        "        self.dropout = config[\"dropout\"]\n",
        "\n",
        "\n",
        "        self.masked_self_attention = MultiHeadAttention(\n",
        "            d_model=self.d_model,\n",
        "            num_heads=self.decoder_attention_heads,\n",
        "            dropout=self.dropout,\n",
        "        )\n",
        "\n",
        "        self.cross_attention = MultiHeadAttention(\n",
        "            d_model=self.d_model,\n",
        "            num_heads=self.decoder_attention_heads,\n",
        "            dropout=self.dropout,\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(self.d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.d_model)\n",
        "        self.layer_norm3 = nn.LayerNorm(self.d_model)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.d_model, self.decoder_ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.decoder_ff_dim, self.d_model),\n",
        "            nn.Dropout(self.dropout)\n",
        "        )\n",
        "    def forward(self,\n",
        "                x,\n",
        "                encoder_output,\n",
        "                self_attention_mask=None,\n",
        "                cross_attention_mask=None):\n",
        "        \"\"\"\n",
        "        we need both the self attention and cross attention mask,\n",
        "        since the decoder uses its Query embeddings to extract information from the\n",
        "        Encoders output and the cross attention mask is needed to prevent the decoder\n",
        "        from attending to 'padding tokens' in the encoder outputs. While the self attention mask\n",
        "        is 'causal' in nature. We ensure that the decoder doesnot peek into the future tokens.\n",
        "        Args:\n",
        "            encoder_output: (batch_size, seq_len, d_model)\n",
        "            self_attention_mask: (batch_size, seq_len, seq_len)\n",
        "            cross_attention_mask: (batch_size, seq_len, seq_len)\n",
        "        Returns:\n",
        "            (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x, _ = self.masked_self_attention(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            mask=self_attention_mask\n",
        "        )\n",
        "        x = x + residual\n",
        "\n",
        "        # important thing to notice here is that the:\n",
        "        # query is coming from the decoder itself\n",
        "        # key and value are coming from the encoder\n",
        "        # so basically decoder is looking at itself and attending to the encoder outputs\n",
        "        # this is why it is called \"cross attention\"\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x, _ = self.cross_attention(\n",
        "            query=x,\n",
        "            key=encoder_output,\n",
        "            value=encoder_output,\n",
        "            mask = cross_attention_mask\n",
        "        )\n",
        "        x = x + residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = x + residual\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqCJdyvmc9AX"
      },
      "source": [
        "### transformer (~ it is basically collection of encoder and decoder blocks stacked up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5TYRWvHc4X4"
      },
      "outputs": [],
      "source": [
        "class BartEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully stacked with encoder blocks\n",
        "    A single encoder block is defined in encoder.py\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = config[\"d_model\"]\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.max_seq_len = config[\"max_seq_len\"]\n",
        "        self.encoder_layers = config[\"encoder_layers\"]\n",
        "        self.dropout = config[\"dropout\"]\n",
        "\n",
        "        self.embedding = TokenEmbedding(\n",
        "            vocab_size=self.vocab_size,\n",
        "            embedding_dim=self.d_model\n",
        "        )\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            embedding_dim=self.d_model,\n",
        "            max_sequence_length=self.max_seq_len,\n",
        "            dropout_prob=self.dropout\n",
        "        )\n",
        "\n",
        "        # stack the encoder blocks\n",
        "        self.encoder_stacked_layers = nn.ModuleList([\n",
        "            EncoderBlock(config) for _ in range(self.encoder_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "            attention_mask: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for layer in self.encoder_stacked_layers:\n",
        "            x = layer(x, attention_mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "class BartDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully stacked with decoder blocks\n",
        "    A single decoder block is defined in decoder.py\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 config):\n",
        "        super().__init__()\n",
        "        self.d_model = config[\"d_model\"]\n",
        "        self.vocab_size = config[\"vocab_size\"]\n",
        "        self.max_seq_len = config[\"max_seq_len\"]\n",
        "        self.decoder_layers = config[\"decoder_layers\"]\n",
        "        self.dropout = config[\"dropout\"]\n",
        "\n",
        "        self.embedding = TokenEmbedding(\n",
        "            vocab_size=self.vocab_size,\n",
        "            embedding_dim=self.d_model\n",
        "        )\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            embedding_dim=self.d_model,\n",
        "            max_sequence_length=self.max_seq_len,\n",
        "            dropout_prob=self.dropout\n",
        "        )\n",
        "\n",
        "        self.decoder_stacked_layers = nn.ModuleList([\n",
        "            DecoderBlock(config) for _ in range(self.decoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.final_layer_norm = nn.LayerNorm(self.d_model)\n",
        "        self.final_dropout = nn.Dropout(self.dropout)\n",
        "        self.output_projection = nn.Linear(self.d_model, self.vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                encoder_output,\n",
        "                self_attention_mask=None,\n",
        "                cross_attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "            encoder_output: (batch_size, seq_len, d_model)\n",
        "            self_attention_mask: (batch_size, seq_len, seq_len)\n",
        "            cross_attention_mask: (batch_size, seq_len, seq_len)\n",
        "        Returns:\n",
        "            (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for layer in self.decoder_stacked_layers:\n",
        "            x = layer(x, encoder_output, self_attention_mask, cross_attention_mask)\n",
        "\n",
        "        x = self.final_layer_norm(x)\n",
        "        x = self.final_dropout(x)\n",
        "        x = self.output_projection(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAZ_vrlFdSik"
      },
      "source": [
        "### sampler, two imples\n",
        "1. Greedy Gen\n",
        "2. Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZGg4naq4Cp1"
      },
      "outputs": [],
      "source": [
        "class GreedyGenerator:\n",
        "    \"\"\"\n",
        "    Simple greedy decoding for autoregressive generation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.max_seq_len = config[\"max_seq_len\"]\n",
        "        self.begin_sequence_token_id = config[\"begin_sequence_token_id\"]\n",
        "        self.end_sequence_token_id = config[\"end_sequence_token_id\"]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        generate sequences using greedy decoding.\n",
        "        Args:\n",
        "            input_ids: (batch_size, seq_len)\n",
        "            attention_mask: (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            generated token ids (batch_size, until end of sequence)\n",
        "        \"\"\"\n",
        "        encoder_output = self.model.encoder(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        batch_size = input_ids.shape[0]\n",
        "        enc_seq_len = encoder_output.size(1)\n",
        "\n",
        "        # start with BOS token\n",
        "        curr_ids = torch.full(\n",
        "            (batch_size, 1),\n",
        "            self.begin_sequence_token_id,\n",
        "            dtype=torch.long,\n",
        "            device=input_ids.device\n",
        "        )\n",
        "        done = [False for _ in range(batch_size)]\n",
        "\n",
        "        # generate tokens one by one until EOS or max seq len limit\n",
        "        step = 0\n",
        "        while not all(done) and step < self.max_seq_len -1 :\n",
        "            step += 1\n",
        "            curr_len = curr_ids.size(1)\n",
        "\n",
        "            # causal mask for decoder\n",
        "            causal_mask = self._get_causal_mask(curr_ids)\n",
        "            # cross-attention mask\n",
        "            if attention_mask is not None:\n",
        "                cross_mask = attention_mask.unsqueeze(1).unsqueeze(2).expand(-1, -1, curr_len, -1)\n",
        "            else:\n",
        "                cross_mask = None\n",
        "\n",
        "            decoder_outputs = self.model.decoder(\n",
        "                curr_ids,\n",
        "                encoder_output=encoder_output,\n",
        "                self_attention_mask=causal_mask,\n",
        "                cross_attention_mask=cross_mask\n",
        "            )\n",
        "\n",
        "            next_token_logits = decoder_outputs[:, -1, :]\n",
        "            # get the most likely token\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
        "            curr_ids = torch.cat([curr_ids, next_token], dim=1)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if next_token[i, 0] == self.end_sequence_token_id:\n",
        "                    done[i] = True\n",
        "\n",
        "            if all(done):\n",
        "                print(f\"All sequences completed at step {step}\")\n",
        "\n",
        "        # remove BOS token if it's at the beginning\n",
        "        final_outputs = []\n",
        "        for i in range(batch_size):\n",
        "            seq = curr_ids[i]\n",
        "            if seq[0] == self.begin_sequence_token_id:\n",
        "                seq = seq[1:]\n",
        "\n",
        "            if len(seq) == 0:\n",
        "                seq = torch.tensor([0], device=seq.device)\n",
        "\n",
        "            final_outputs.append(seq)\n",
        "\n",
        "        return torch.stack(final_outputs)\n",
        "\n",
        "    def _get_causal_mask(self, input_ids):\n",
        "        \"\"\"a causal mask for decoder no peeking\"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # a square mask where the upper triangle is True (will be masked)\n",
        "        mask = torch.triu(\n",
        "            torch.ones((seq_len, seq_len), dtype=torch.bool),\n",
        "            diagonal=1\n",
        "        ).to(input_ids.device)\n",
        "\n",
        "        # mask with dimensions [batch_size, 1, seq_len, seq_len]\n",
        "        mask = mask.unsqueeze(0).unsqueeze(1).expand(batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "        # convert from bool mask to 0/1 mask as expected by the attention\n",
        "        mask = ~mask  # invert since triu gives the part to mask out\n",
        "\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1wuFH37dMmN"
      },
      "outputs": [],
      "source": [
        "class BeamSearchGenerator:\n",
        "    \"\"\"\n",
        "    Beam search for autoregressive decoding.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 config):\n",
        "        self.model = model\n",
        "        self.beam_size = config[\"beam_size\"]\n",
        "        self.max_seq_len = config[\"max_seq_len\"]\n",
        "        self.pad_token_id = config[\"pad_token_id\"]\n",
        "        self.begin_sequence_token_id = config[\"begin_sequence_token_id\"]\n",
        "        self.end_sequence_token_id = config[\"end_sequence_token_id\"]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self,\n",
        "                 input_ids,\n",
        "                 attention_mask=None):\n",
        "        \"\"\"\n",
        "        Generate sequences using beam search.\n",
        "        Args:\n",
        "            input_ids: (batch_size, seq_len)\n",
        "            attention_mask: (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            Generated Token ids (batch_size, until end of sequence)\n",
        "        \"\"\"\n",
        "        encoder_output = self.model.encoder(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        batch_size = input_ids.shape[0]\n",
        "        enc_seq_len = encoder_output.size(1)\n",
        "\n",
        "        curr_ids = torch.full(\n",
        "            (batch_size, 1),\n",
        "            self.begin_sequence_token_id,\n",
        "            dtype=torch.long,\n",
        "            device=input_ids.device\n",
        "        )\n",
        "\n",
        "        beam_scores = torch.zeros(\n",
        "            (batch_size, self.beam_size),\n",
        "            dtype=torch.float,\n",
        "            device=input_ids.device\n",
        "        )\n",
        "\n",
        "        # for beam search, expand each sequence in batch to beam_size copies\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, beam_size, seq_len, d_model]\n",
        "        # use repeat instead of expand to ensure contiguous memory\n",
        "        encoder_outputs = encoder_output.unsqueeze(1).repeat(1, self.beam_size, 1, 1)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [batch_size, seq_len] -> [batch_size, beam_size, 1, seq_len]\n",
        "            cross_mask = attention_mask.unsqueeze(1).unsqueeze(2).repeat(1, self.beam_size, 1, 1)\n",
        "        else:\n",
        "            # create a default mask that allows attending to all encoder positions (in case we don't have a mask)\n",
        "            cross_mask = torch.ones(\n",
        "                (input_ids.size(0), self.beam_size, 1, encoder_output.size(1)),\n",
        "                device=input_ids.device\n",
        "            )\n",
        "\n",
        "        done_beams = [[False for _ in range(self.beam_size)] for _ in range(batch_size)]\n",
        "\n",
        "        # shape: [batch_size * beam_size, 1]\n",
        "        curr_ids = curr_ids.repeat(self.beam_size, 1)\n",
        "\n",
        "        step = 0\n",
        "        while not all(all(done) for done in done_beams) and step < self.max_seq_len:\n",
        "            step += 1\n",
        "\n",
        "            # reshape encoder outputs to match batch_size * beam_size\n",
        "            flat_encoder_outputs = encoder_outputs.reshape(\n",
        "                batch_size * self.beam_size,\n",
        "                enc_seq_len,\n",
        "                -1\n",
        "            )\n",
        "\n",
        "            if cross_mask is not None:\n",
        "                # [batch_size, beam_size, 1, enc_seq_len] -> [batch_size, beam_size, curr_seq_len, enc_seq_len]\n",
        "                curr_cross_mask = cross_mask.expand(-1, -1, curr_ids.size(1), -1)\n",
        "                # reshape to [batch_size * beam_size, 1, curr_seq_len, enc_seq_len]\n",
        "                curr_cross_mask = curr_cross_mask.reshape(\n",
        "                    batch_size * self.beam_size,\n",
        "                    1,\n",
        "                    curr_ids.size(1),\n",
        "                    enc_seq_len\n",
        "                )\n",
        "            else:\n",
        "                curr_cross_mask = None\n",
        "\n",
        "            decoder_outputs = self.model.decoder(\n",
        "                curr_ids,\n",
        "                encoder_output=flat_encoder_outputs,\n",
        "                self_attention_mask=self._get_causal_mask(curr_ids),\n",
        "                cross_attention_mask=curr_cross_mask\n",
        "            )\n",
        "\n",
        "            next_token_logits = decoder_outputs[:, -1, :]\n",
        "            next_token_scores = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "            next_token_scores = next_token_scores.reshape(batch_size, self.beam_size, -1)\n",
        "\n",
        "            next_scores = beam_scores.unsqueeze(-1) + next_token_scores\n",
        "            next_scores = next_scores.reshape(batch_size, -1)\n",
        "\n",
        "            # select top-k scores and their indices\n",
        "            topk_scores, topk_indices = next_scores.topk(self.beam_size, dim=1)\n",
        "\n",
        "            beam_indices = topk_indices // next_token_scores.size(-1)\n",
        "            token_indices = topk_indices % next_token_scores.size(-1)\n",
        "            beam_scores = topk_scores\n",
        "\n",
        "            next_ids = []\n",
        "            for batch_idx in range(batch_size):\n",
        "                batch_next_ids = []\n",
        "\n",
        "                for beam_idx in range(self.beam_size):\n",
        "                    # skip if this beam is already done\n",
        "                    if done_beams[batch_idx][beam_idx]:\n",
        "                        # just copy the existing sequence\n",
        "                        curr_beam_idx = beam_indices[batch_idx, beam_idx]\n",
        "                        curr_seq = curr_ids[batch_idx * self.beam_size + curr_beam_idx].clone()\n",
        "                        batch_next_ids.append(curr_seq)\n",
        "                        continue\n",
        "\n",
        "                    curr_beam_idx = beam_indices[batch_idx, beam_idx]\n",
        "                    curr_seq = curr_ids[batch_idx * self.beam_size + curr_beam_idx].clone()\n",
        "                    new_token = token_indices[batch_idx, beam_idx].unsqueeze(0)\n",
        "                    next_seq = torch.cat([curr_seq, new_token], dim=0)\n",
        "                    batch_next_ids.append(next_seq)\n",
        "\n",
        "                    if new_token.item() == self.end_sequence_token_id:\n",
        "                        done_beams[batch_idx][beam_idx] = True\n",
        "\n",
        "                # common bug: make sure all sequences in batch_next_ids have the same length before stacking\n",
        "                max_len = max([seq.size(0) for seq in batch_next_ids])\n",
        "                padded_batch_next_ids = []\n",
        "\n",
        "                for seq in batch_next_ids:\n",
        "                    if seq.size(0) < max_len:\n",
        "                        padding = torch.full(\n",
        "                            (max_len - seq.size(0),),\n",
        "                            self.pad_token_id,\n",
        "                            dtype=torch.long,\n",
        "                            device=seq.device\n",
        "                        )\n",
        "                        padded_seq = torch.cat([seq, padding], dim=0)\n",
        "                        padded_batch_next_ids.append(padded_seq)\n",
        "                    else:\n",
        "                        padded_batch_next_ids.append(seq)\n",
        "\n",
        "                next_ids.append(torch.stack(padded_batch_next_ids))\n",
        "\n",
        "            # stack and reshape token ids for next iteration\n",
        "            next_ids = torch.stack(next_ids)  # [batch_size, beam_size, seq_len]\n",
        "            curr_ids = next_ids.reshape(batch_size * self.beam_size, -1)\n",
        "\n",
        "        # return the top beam for each sequence in batch\n",
        "        final_outputs = []\n",
        "        for batch_idx in range(batch_size):\n",
        "            # get the beam with highest score\n",
        "            best_beam_idx = torch.argmax(beam_scores[batch_idx])\n",
        "            best_seq = curr_ids[batch_idx * self.beam_size + best_beam_idx]\n",
        "\n",
        "            # when presenting the output, we don't want to include the BOS token\n",
        "            if best_seq[0] == self.begin_sequence_token_id:\n",
        "                best_seq = best_seq[1:]\n",
        "\n",
        "            # when presenting the output, we don't want to include the EOS token\n",
        "            if self.end_sequence_token_id in best_seq:\n",
        "                # find the first occurrence of EOS and truncate\n",
        "                eos_idx = (best_seq == self.end_sequence_token_id).nonzero(as_tuple=True)[0]\n",
        "                if len(eos_idx) > 0:\n",
        "                    best_seq = best_seq[:eos_idx[0]]\n",
        "\n",
        "            if len(best_seq) == 0:\n",
        "                best_seq = torch.tensor([0], device=best_seq.device)\n",
        "\n",
        "            final_outputs.append(best_seq)\n",
        "\n",
        "        return torch.stack(final_outputs)\n",
        "\n",
        "    def _get_causal_mask(self, input_ids):\n",
        "        \"\"\"causal mask for decoder no peeking \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # a square mask where the upper triangle is True (will be masked)\n",
        "        mask = torch.triu(\n",
        "            torch.ones((seq_len, seq_len), dtype=torch.bool),\n",
        "            diagonal=1\n",
        "        ).to(input_ids.device)\n",
        "\n",
        "        # mask with dimensions [batch_size, 1, seq_len, seq_len]\n",
        "        # the '1' dimension corresponds to the attention heads\n",
        "        mask = mask.unsqueeze(0).unsqueeze(1).expand(batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "        # convert from bool mask to 0/1 mask as expected by the attention\n",
        "        # 0 means masked positions (don't attend here), 1 means valid positions\n",
        "        # we have to invert the mask because the triu function gives the part to mask out\n",
        "        mask = ~mask\n",
        "\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL7oBQ6sfUNx"
      },
      "source": [
        "### engine: combines the BART model in one place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPF7J6xlzjLm"
      },
      "outputs": [],
      "source": [
        "class BART(nn.Module):\n",
        "    \"\"\"\n",
        "    BART: Denoising Seq-to-Seq Pre-training for\n",
        "    Natural Language Generation, Translation, and Comprehension\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.max_seq_len = config[\"max_seq_len\"]\n",
        "        self.encoder = BartEncoder(config)\n",
        "        self.decoder = BartDecoder(config)\n",
        "        # Tie the decoder embedding and output projection layer weights\n",
        "        # Reason ?\n",
        "        # 1. Reduce model params\n",
        "        # 2. Act as a regularizer\n",
        "        # 3. Since it is just an inverse operation (embedding: input tokens -> embedding, projection: embedding -> output tokens)\n",
        "        self.decoder.output_projection.weight = self.decoder.embedding.embedding.weight\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids,\n",
        "                decoder_input_ids,\n",
        "                encoder_padding_mask=None,\n",
        "                encoder_decoder_attention_mask=None,\n",
        "                decoder_causal_mask=None,\n",
        "                labels=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: (batch_size, seq_len)\n",
        "            decoder_input_ids: (batch_size, seq_len)\n",
        "            encoder_padding_mask: (batch_size, seq_len)\n",
        "            encoder_decoder_attention_mask: (batch_size, seq_len, seq_len)\n",
        "            decoder_causal_mask: (batch_size, seq_len, seq_len)\n",
        "            labels: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            (outputs, loss) if labels are provided else (output tokens)\n",
        "            outputs: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        encoder_output = self.encoder(input_ids,\n",
        "                                      attention_mask=encoder_padding_mask)\n",
        "        decoder_output = self.decoder(decoder_input_ids,\n",
        "                                      encoder_output=encoder_output,\n",
        "                                      self_attention_mask=decoder_causal_mask,\n",
        "                                      cross_attention_mask=encoder_decoder_attention_mask)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                # (batch_size * seq_len, vocab_size)\n",
        "                decoder_output.view(-1, decoder_output.size(-1)),\n",
        "                # (batch_size * seq_len)\n",
        "                labels.view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "            return decoder_output, loss\n",
        "\n",
        "        return decoder_output\n",
        "\n",
        "    def generate(self,\n",
        "                 input_ids,\n",
        "                 use_greedy,\n",
        "                 attention_mask=None):\n",
        "        '''\n",
        "        This method will be used to generate summaries using the beam search algo that we implemented\n",
        "\n",
        "        ArgS:\n",
        "            input_ids: input token ids (batch size, seq len)\n",
        "            attention_mask: Attention mask for the input (batch size, seq len)\n",
        "        '''\n",
        "        if use_greedy:\n",
        "            greedy_generator = GreedyGenerator(self, self.config)\n",
        "            return greedy_generator.generate(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "        else:\n",
        "            beam_generator = BeamSearchGenerator(self, self.config)\n",
        "            return beam_generator.generate(input_ids, attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OylyWkhhVRK"
      },
      "source": [
        "### training utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYdTIdZ7ibXM"
      },
      "outputs": [],
      "source": [
        "def calculate_rouge(predictions, references):\n",
        "    rouge = evaluation_metrics.load(\"rouge\")\n",
        "\n",
        "    results = rouge.compute(\n",
        "        predictions=predictions,\n",
        "        references=references,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    return {k: round(v,4) for k,v in results.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TgBiTaYhOLD"
      },
      "outputs": [],
      "source": [
        "def train(save_dir=\"checkpoints\"):\n",
        "    \"\"\"\n",
        "    train bart model on the cnn/dailymail dataset\n",
        "\n",
        "    Args:\n",
        "        config_path: model configuration yaml file\n",
        "        batch_size: batch size for training\n",
        "        num_epochs: number of training epochs\n",
        "        learning_rate: learning rate for the optimizer\n",
        "        warmup_steps: number of warmup steps for the learning rate scheduler\n",
        "        max_grad_norm: maximum gradient norm for gradient clipping\n",
        "        save_dir: directory to save model checkpoints\n",
        "    \"\"\"\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    batch_size = model_config[\"batch_size\"]\n",
        "    max_seq_len = model_config[\"max_seq_len\"]\n",
        "    num_epochs = model_config[\"num_epochs\"]\n",
        "    learning_rate = model_config[\"learning_rate\"]\n",
        "    warmup_steps = model_config[\"warmup_steps\"]\n",
        "    max_grad_norm = model_config[\"max_grad_norm\"]\n",
        "    log_every = model_config[\"log_every\"]\n",
        "    eval_sample = model_config[\"eval_sample\"]\n",
        "    start_eval_gen = model_config[\"start_eval_gen\"]\n",
        "\n",
        "    train_dataloader, val_dataloader, test_dataloader = load_cnn_dailymail(batch_size=batch_size,max_length=max_seq_len)\n",
        "    model = BART(model_config)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total number of parameters: {total_params:,}\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer=optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    total_steps = len(train_dataloader)*num_epochs\n",
        "\n",
        "    scheduler=get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # we will keep track of imp metrics and also during the training process\n",
        "    # we will generate summaries (for vibe checks) and to calculate rouge scores\n",
        "    metrics_history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"perplexity\": [],\n",
        "        \"rouge1\": [],\n",
        "        \"rouge2\": [],\n",
        "        \"rougeL\": [],\n",
        "        \"learning_rate\": []\n",
        "    }\n",
        "\n",
        "    global_step = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        epoch_step = 0\n",
        "\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            seq_len = decoder_input_ids.size(1)\n",
        "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "            # create a cross attention mask with dim [batch_size, 1, decoder_seq_len, encoder_seq_len]\n",
        "            # this will allow the decoder position to attend to all encoder positions\n",
        "            # Note: we use the encoder's attention mask, not decoder attention mask because\n",
        "            # we need to prevent attending to padding tokens in the encoder sequence\n",
        "            encoder_decoder_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).expand(-1,-1,seq_len,-1)\n",
        "\n",
        "            # forward pass with causal mask\n",
        "            _,loss = model(\n",
        "                input_ids=input_ids,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                encoder_padding_mask=attention_mask,\n",
        "                encoder_decoder_attention_mask=encoder_decoder_attention_mask,\n",
        "                decoder_causal_mask=causal_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            # backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # compute perplexity = exp(loss)\n",
        "            perplexity = math.exp(loss.item())\n",
        "            total_loss += loss.item()\n",
        "            epoch_step += 1\n",
        "            global_step += 1\n",
        "\n",
        "            progress_bar.set_postfix(\n",
        "                {\n",
        "                    \"step\": global_step,\n",
        "                    \"loss\": loss.item(),\n",
        "                    \"avg_loss\": total_loss/(epoch_step + 1),\n",
        "                    \"ppl\": perplexity,\n",
        "                    \"lr\": scheduler.get_last_lr()[0]\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if global_step % log_every == 0:\n",
        "                metrics_history[\"train_loss\"].append((global_step, loss.item()))\n",
        "                metrics_history[\"perplexity\"].append((global_step, perplexity))\n",
        "                metrics_history[\"learning_rate\"].append((global_step, scheduler.get_last_lr()[0]))\n",
        "\n",
        "                # after waiting for start_eval_gen steps, we will generate some summaries\n",
        "                if global_step > start_eval_gen:\n",
        "                    print(\"\\n---Sample Generation---\")\n",
        "                    sample_input = input_ids[0:1]\n",
        "                    sample_mask = attention_mask[0:1]\n",
        "\n",
        "                    generated_ids = model.generate(\n",
        "                        input_ids=sample_input,\n",
        "                        attention_mask=sample_mask,\n",
        "                        use_greedy=True\n",
        "                    )\n",
        "\n",
        "                    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "                    original_text = tokenizer.batch_decode(sample_input, skip_special_tokens=True)\n",
        "\n",
        "                    print(f\"Original: {original_text[0]}\")\n",
        "                    print(f\"Generated: {generated_text[0]}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_step = 0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        # lets do some eval\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "                decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                seq_len = decoder_input_ids.size(1)\n",
        "                causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "                # create a cross attention mask with dim [batch_size, 1, decoder_seq_len, encoder_seq_len]\n",
        "                # this will allow the decoder position to attend to all encoder positions\n",
        "                encoder_seq_len = input_ids.size(1)\n",
        "                encoder_decoder_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2).expand(-1,-1,seq_len,-1)\n",
        "\n",
        "                # forward pass\n",
        "                _,loss = model(\n",
        "                    input_ids=input_ids,\n",
        "                    decoder_input_ids=decoder_input_ids,\n",
        "                    encoder_padding_mask=attention_mask,\n",
        "                    encoder_decoder_attention_mask=encoder_decoder_attention_mask,\n",
        "                    decoder_causal_mask=causal_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_step += 1\n",
        "\n",
        "                if val_step <= eval_sample:\n",
        "                    generated_ids = model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        use_greedy=True\n",
        "                    )\n",
        "\n",
        "                    generated_summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "                    reference_summaries = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "                    all_preds.extend(generated_summaries)\n",
        "                    all_targets.extend(reference_summaries)\n",
        "\n",
        "        avg_val_loss = val_loss / val_step\n",
        "        val_perplexity = math.exp(avg_val_loss)\n",
        "        if all_preds:\n",
        "            rouge_scores = calculate_rouge(all_preds, all_targets)\n",
        "#            print(rouge_scores)\n",
        "            rouge1 = rouge_scores['rouge1']\n",
        "            rouge2 = rouge_scores['rouge2']\n",
        "            rougel = rouge_scores['rougeL']\n",
        "\n",
        "            metrics_history['rouge1'].append((global_step, rouge1))\n",
        "            metrics_history['rouge2'].append((global_step, rouge2))\n",
        "            metrics_history['rougeL'].append((global_step, rougel))\n",
        "\n",
        "            print(f\"\\nROUGE Scores - R1: {rouge1:.4f}, R2: {rouge2:.4f}, RL: {rougel:.4f}\")\n",
        "\n",
        "        metrics_history['val_loss'].append((global_step, avg_val_loss))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Training Loss: {total_loss/len(train_dataloader):.4f}\")\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Validation Perplexity: {val_perplexity:.4f}\")\n",
        "\n",
        "        checkpoint_path = os.path.join(save_dir, f\"epoch_{epoch+1}.pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': total_loss/len(train_dataloader),\n",
        "            'val_loss': avg_val_loss,\n",
        "            'metrics': metrics_history\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_loss': total_loss/len(train_dataloader),\n",
        "                'val_loss': avg_val_loss,\n",
        "                'metrics': metrics_history\n",
        "            }, best_model_path)\n",
        "            print(f\"New best model saved with val_loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        print(f\"Model saved to {checkpoint_path}\")\n",
        "\n",
        "        if all_preds:\n",
        "            print(\"\\nSample Generations:\")\n",
        "            for i in range(min(100, len(all_preds))):\n",
        "                print(f\"\\nGenerated: {all_preds[i]}...\")\n",
        "                print(f\"Reference: {all_targets[i]}...\")\n",
        "\n",
        "    print(\"Training complete\")\n",
        "\n",
        "    import json\n",
        "    metrics_path = os.path.join(save_dir, \"training_metrics.json\")\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics_history, f)\n",
        "    print(f\"Training metrics saved to {metrics_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAM16NYKizoU"
      },
      "source": [
        "### evaluation metric (rouge score)\n",
        "[click to watch this video for better understanding on how the rouge score is calculated](https://www.youtube.com/watch?v=TMshhnrEXlg&ab_channel=HuggingFace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4nUunEjjGI9"
      },
      "outputs": [],
      "source": [
        "def evaluate(checkpoint_path):\n",
        "    '''\n",
        "    evaluate the bart model on the cnn/dailymail dataset\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: path to the checkpoint file\n",
        "        config_path: path to the config file\n",
        "        batch_size: batch size for evaluation\n",
        "        num_samples: number of samples to evaluate on\n",
        "    '''\n",
        "    batch_size = model_config[\"batch_size\"]\n",
        "    max_seq_len = model_config[\"max_seq_len\"]\n",
        "    num_samples = model_config[\"num_samples\"]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    _,_,test_dataloader = load_cnn_dailymail(batch_size=batch_size, max_length=max_seq_len)\n",
        "\n",
        "    model = BART(model_config)\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "    all_preds, all_targets, all_articles = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(test_dataloader, desc=\"Generating Summaries\")):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                use_greedy=True\n",
        "            )\n",
        "\n",
        "            generated_summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            reference_summaries = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "            articles = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "\n",
        "            all_preds.extend(generated_summaries)\n",
        "            all_targets.extend(reference_summaries)\n",
        "            all_articles.extend(articles)\n",
        "\n",
        "    rouge_scores = calculate_rouge(all_preds, all_targets)\n",
        "\n",
        "    print(f\"ROUGE Scores: \")\n",
        "    print(f\"ROUGE-1: {rouge_scores['rouge1']}\")\n",
        "    print(f\"ROUGE-2: {rouge_scores['rouge2']}\")\n",
        "    print(f\"ROUGE-L: {rouge_scores['rougeL']}\")\n",
        "\n",
        "    print(\"\\n Sample Summaries:\")\n",
        "    for i in range(min(3, len(all_preds))):\n",
        "        print(f\"\\nArticle {i+1}:\")\n",
        "        print(all_articles[i])\n",
        "        print(f\"\\nGenerated Summary: {all_preds[i]}\")\n",
        "        print(f\"Reference Summary: {all_targets[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YGCxNKTkAK-"
      },
      "source": [
        "### lets train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwyX-MCcj7_l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    train(save_dir=\"checkpoints\")\n",
        "    checkpoint_path = \"/content/checkpoints/best_model.pth\"\n",
        "    evaluate(checkpoint_path=checkpoint_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}