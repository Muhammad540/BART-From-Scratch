model_name: "bart_seq2seq"

model_config:
    vocab_size: 50265
    max_seq_len: 512
    d_model: 512
    num_heads: 8
    encoder_layers: 3
    decoder_layers: 3
    encoder_attention_heads: 8
    decoder_attention_heads: 8
    encoder_ff_dim: 1024
    decoder_ff_dim: 1024
    dropout: 0.1
    attention_dropout: 0.1
    activation_dropout: 0.1
    pad_token_id: 1
    begin_sequence_token_id: 0
    end_sequence_token_id: 2
