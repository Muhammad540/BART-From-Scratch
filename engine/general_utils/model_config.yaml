model_name: "bart_seq2seq"

vocab_size: 50265              # defines the size of the token vocab
max_seq_len: 512               # max sequence of length that the model can process
d_model: 512                   # dimension of embedding and hidden states throughout the model
encoder_layers: 3              # number of encoder/decoder layers in the transformer
decoder_layers: 3              # number of encoder/decoder layers in the transformer
encoder_attention_heads: 8     # number of attention heads in multihead attention
decoder_attention_heads: 8     # number of attention heads in multihead attention
encoder_ff_dim: 1024           # hidden dim size in FF network
decoder_ff_dim: 1024           # hidden dim size in FF network
dropout: 0.1                   # general dropout rate throughout the model
pad_token_id: 1                # special token ids used in generation and processing
begin_sequence_token_id: 0     # special token ids used in generation and processing
end_sequence_token_id: 2       # special token ids used in generation and processing
beam_size: 4                   # control the num of paths in beam search
batch_size: 16                 # batch size (how many examples are batched together to train in parallel)
learning_rate: 3.0e-5            # learning rate decides how big of a step you should take in the GD
warmup_steps: 500              # used in the learning rate scheduler
max_grad_norm: 1.0             # maximum gradient norm used for clipping (used in the training loop  )
num_epochs: 3                  # number  full passes through the training data
num_samples: 10                # how many samples to generate in the evaluation (for vibe check)
eval_samples: 3                # num samples that we get in the evaluation during training
log_every: 1000                # defines logging frequency 
start_eval_gen: 100            # again used for logging frequency