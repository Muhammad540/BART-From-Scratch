model_name: "bart_seq2seq"

model_config:
    vocab_size: 50265
    max_seq_len: 512
    d_model: 512
    encoder_layers: 3
    decoder_layers: 3
    encoder_attention_heads: 8
    decoder_attention_heads: 8
    encoder_ff_dim: 1024
    decoder_ff_dim: 1024
    dropout: 0.1
    attention_dropout: 0.1
    pad_token_id: 1
    begin_sequence_token_id: 0
    end_sequence_token_id: 2
    beam_size: 4
    max_seq_len: 10000

learning_rate: 3e-5
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
weight_decay: 0.01
warmup_steps: 500
max_steps: 100000
max_grad_norm: 1.0
